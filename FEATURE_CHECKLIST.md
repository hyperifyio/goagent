* [x] Length backoff: on truncation (e.g., `finish_reason=="length"`), automatically double the completion cap once (bounded by remaining context) and retry; include local unit tests that simulate truncation; **DoD:** `go test ./...` passes and behavior described in the policy docs.
  - [x] [S01:request-max-tokens-field] Add `MaxTokens int` with `json:"max_tokens,omitempty"` to `internal/oai/types.go` struct `ChatCompletionsRequest`; add `internal/oai/types_serialization_test.go` to assert JSON includes `max_tokens` only when set; **DoD:** tests pass and no breaking changes to existing payload fields.
  - [x] [S02:agent-plumb-cap-field] In `cmd/agentcli/main.go` request assembly, include `req.MaxTokens` only when a local variable `completionCap > 0`; no behavior change yet (default `completionCap=0`); add unit test asserting absence when `completionCap=0`; **DoD:** tests pass and binary behavior unchanged.
  - [x] [S03:length-retry-core] In `cmd/agentcli/main.go` within the per-step call, detect `resp.Choices[0].FinishReason=="length"` and implement a one-time in-step retry guarded by `retriedForLength bool` that sets `completionCap = max(256, completionCap*2)` and resends with `req.MaxTokens=completionCap`; ensure we do not increment the agent step on the retry; **DoD:** logic compiles, guarded by unit tests in next slice.
  - BLOCKED: [S04:test-length-retry-mock] Spec expects second request `max_tokens:512`, but current implementation initializes `completionCap=0`, so first retry sets `max_tokens` to 256 (not 512); proceeding would fail deterministically.
    - Next: Edit this checklist to expect second request `max_tokens:256` (0→256 per S03), then add the test in `cmd/agentcli/main_test.go` asserting exactly two POSTs and `max_tokens:256` on the second request.
  - [x] [S05:context-window-map] Create `internal/oai/context_window.go` with `ContextWindowForModel(model string) int` (defaults to 128000; e.g., `oss-gpt-20b`→8192) and `internal/oai/context_window_test.go` covering the map; **DoD:** tests pass and function usable by clamp logic.
  - [ ] [S06:naive-token-estimator] Add `internal/oai/token_estimate.go` with `EstimateTokens(messages []Message) int` using a simple heuristic (e.g., chars/4 + per-message overhead); include `internal/oai/token_estimate_test.go` asserting monotonic growth and rough scale; **DoD:** tests pass; no external deps.
  - [ ] [S07:cap-clamp] Add `ClampCompletionCap(messages []oai.Message, cap, window int) int` in `internal/oai/context_window.go` to bound `cap` to `max(1, window-EstimateTokens(messages)-32)`; include unit tests; **DoD:** tests pass and clamp is deterministic.
  - [ ] [S08:integrate-clamp] In `cmd/agentcli/main.go` length-retry path, compute `window := oai.ContextWindowForModel(cfg.model)` then set `completionCap = oai.ClampCompletionCap(messages, completionCap, window)` before resend; add `TestLengthBackoff_ClampDoesNotExceedWindow` to assert we never exceed remaining context by inspecting second request `max_tokens`; **DoD:** tests pass.
  - [ ] [S09:audit-length-backoff] Emit an NDJSON audit entry on length backoff using existing `internal/oai/client.go:appendAuditLog` mechanism (event `"length_backoff"`, fields: `model`, `prev_cap`, `new_cap`, `window`, `estimated_prompt_tokens`); unit test writes to a temp module root by `chdir` and asserts one log line with expected fields; **DoD:** test passes on all platforms.
  - [ ] [S10:edge-cases] Add tests in `cmd/agentcli/main_test.go` verifying (a) no retry when `FinishReason!="length"`, (b) only one retry even if the second response is also `"length"` (proceed without further retries), and (c) length backoff does not interfere with tool_call flow; **DoD:** tests pass and existing tool-call tests remain green.
* [ ] Agent loop safety: ensure the default `-max-steps` is 8 with a hard ceiling of 15 and terminate with a clear “needs human review” message when the cap is hit; **DoD:** local unit test drives the loop to the cap and asserts the message, and the README mentions the guard.
* [ ] HTTP timeouts and retries: wire jittered exponential backoff for 429/5xx/timeouts and use a sane default overall timeout (minutes, not seconds); add local unit tests that verify the retry schedule and backoff growth; **DoD:** `go test ./...` passes and the README’s defaults match observed behavior.
* [ ] README defaults: update the “Common flags” table so defaults (notably `-temp 1.0` and `-max-steps 8`) match the binary, and add a short “Why you usually don’t need to change knobs” section pointing to the policy; **DoD:** README renders locally and manual `./agentcli -h` matches the table.
* [ ] GPT-5 live smoke: add a local smoke test that runs `./agentcli --model gpt-5` with **no sampling flags** and asserts that the request includes `temperature:1` while allowing `verbosity`/`reasoning_effort` to be set; **DoD:** test passes locally when `GPT5_OPENAI_API_URL` and `GPT5_OPENAI_API_KEY` are exported.
* [ ] GPT-5 mock smoke: add a local integration test using a mock OpenAI-compatible endpoint that asserts `temperature:1` is sent by default and that reasoning controls can be toggled without altering temperature; **DoD:** mock test passes locally and the docs include a “Zero-config with GPT-5” example.
